import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
data = pd.read_csv("/content/program_5_dataset.csv")
data.shape

data.head()
data.isnull().sum()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Sex']=le.fit_transform(data['Sex'])
# data.replace({'Sex':{'m':0,'f':1}},inplace=True)
data.head()
data['ALB'].fillna(data['ALB'].mean(),inplace=True)
data['ALP'].fillna(data['ALP'].mode()[0],inplace=True)
data['ALP'].mode()[0]
data.head()
data = data.dropna()
data.isnull().sum()
data.head()
data.shape
df1 = data.drop(columns=['ALT','CHE'],axis=1)
df1.head()
df1['Sex'].value_counts()
x=df1['ALP']
y=df1['PROT']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=33)
mean_value = x_train.mean()
mean_value=np.array(mean_value)
plt.scatter(x_train,y_train,label='Actual DataPoints',color='purple',marker="^")
plt.axhline(y=mean_value,color='red',label='MeanValue of Target')
plt.xlabel('ALP')
plt.ylabel('PROT')
plt.annotate('Mean Line',xy=(150,mean_value+1))
plt.title('TrainData wrt MeanLine')
plt.legend()
plt.show()
# n=len(x)
# split = int(0.7*n)
# x_train, x_test = x[:split],x[split:]
# y_train, y_test = y[:split],y[split:]
xy = x_train*y_train
n=len(x_train)

x_mean=x_train.mean()
y_mean=y_train.mean()

numerator = xy.sum()-(n*x_mean*y_mean)
denominator = (x_train**2).sum() - (n*(x_mean**2))

m = numerator/denominator
c = y_mean - (m*x_mean)

print("Intercept : ",c)
print("Coefficient : ",m)
x_train_a=np.array(x_train).reshape(-1,1)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train_a,y_train)

print("Intercept : ",model.intercept_)
print("Coefficient : ",model.coef_)
n=len(x_train)
df2 = df1.iloc[:n,:]
y_pred = c+m*df1.iloc[:n,5]
df2['PredPROT']=y_pred
df2[['ALP','PROT','PredPROT']]
fig,ax=plt.subplots()
ax.scatter(x='ALP',y='PROT',data=df2,label='Actual Values')
ax.plot(df2['ALP'],df2['PredPROT'],color='red',label='BestFitTrainingLine')
ax.set_xlabel('ALP')
ax.set_ylabel('PROT')
ax.set_title("BestFitModelLine on Training Data")
ax.legend()
plt.show()
df3 = pd.DataFrame({'ALP':df2['ALP'],'PROT':df2['PROT'],'PredPROT':df2['PredPROT'],'Error':(df2.PredPROT-df2.PROT)})
df3.head()
MSE = (df3['Error']**2).sum()/n
MSE
SSE = (df3['Error']**2).sum()
SST = ((df3['PROT']-df3['PROT'].mean())**2).sum()
SSR = SST - SSE
R2 = SSR/SST
R2
from sklearn.metrics import r2_score
adjusted_r2 = 1 - (1 - r2_score(df3['PROT'], df3['PredPROT'])) * (len(df3['PROT']) - 1) / (len(df3['PROT']) - df3.shape[1] - 1)
adjusted_r2
n=len(x)-len(x_train)
df4 = df1.iloc[n:,:]
y_pred = c+m*df1.iloc[n:,5]
df4['PredPROT']=y_pred
df4[['ALP','PROT','PredPROT']]
fig,ax=plt.subplots()
ax.scatter(x='ALP',y='PROT',data=df4,label='Actual Values')
ax.plot(df2['ALP'],df2['PredPROT'],color='red',label='BestFitTestingLine')
ax.set_xlabel('ALP')
ax.set_ylabel('PROT')
ax.set_title("BestFitModelLine on Testing Data")
ax.legend()
plt.show()
df5 = pd.DataFrame({'ALP':df4['ALP'],'PROT':df4['PROT'],'PredPROT':df4['PredPROT'],'Error':(df4.PredPROT-df4.PROT)})
df5.head()
y_pred = model.predict(np.array(x_test).reshape(-1,1))
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
adjusted_r2 = 1 - (1 - r2_score(df3['PROT'], df3['PredPROT'])) * (len(df5['PROT']) - 1) / (len(df5['PROT']) - df5.shape[1] - 1)
print(f'Mean Squared Error (MSE): {mse}\nR-squared (R2): {r2}\nAdjusted R-squared: {adjusted_r2}')
--------------------------------------------------------------------------------
from statsmodels.stats.outliers_influence import variance_inflation_factor
x = data[['Age','Sex','ALT','BIL']]
y = data['PROT']
vif = pd.Series([variance_inflation_factor(x.values,idx)
                 for idx in range(x.shape[1])],
                index=x.columns)
print(vif)
from statsmodels.stats.outliers_influence import variance_inflation_factor
x = data[['BIL','ALT']]
y = data['PROT']
vif = pd.Series([variance_inflation_factor(x.values,idx)
                 for idx in range(x.shape[1])],
                index=x.columns)
print(vif)
np.corrcoef(data['BIL'],data['ALT'])
plt.scatter(data['BIL'],data['PROT'],color='green')
plt.xlabel('BIL')
plt.ylabel('PROT')
plt.title("Association btw BIL and PROT")
plt.show()
plt.scatter(data['ALT'],data['PROT'],color='purple')
plt.xlabel('ALT')
plt.ylabel('PROT')
plt.title("Association btw ALT and PROT")
plt.show()
x=data[['BIL','ALT']]
y=data['PROT']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)

print("Intercept : ", model.intercept_)
print("Coefficients : ", model.coef_)
y_pred = model.predict(x_test)
n=len(x_train)
DF = data.iloc[n:,:]
DF['PredPROT'] = y_pred
DF.head()
DF
DF1 = pd.DataFrame({'BIL':DF['BIL'],'ALT':DF['ALT'],'PROT':DF['ALT'],'PredPROT':DF['PredPROT'],'Error':(DF.PredPROT-DF.PROT)})
DF1.head()
from sklearn.metrics import r2_score, mean_squared_error
r2 = r2_score(y_test,y_pred)
mse = mean_squared_error(y_test,y_pred)
adj_r2 = 1 - (1 - r2_score(DF1['PROT'],DF1['PredPROT']))*(len(DF1['PROT'])-1)/len(DF1['PROT']-DF1.shape[1]-1)
print(f"R2 : {r2}, MSE : {mse}, ADJ.R2 : {adj_r2}")
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Assuming DF1 contains the data and 'BIL' and 'ALT' are the predictor variables
X = DF1[['BIL', 'ALT']]
y_observed = DF1['PROT']
y_predicted = DF1['PredPROT']

# Fit a linear regression model
model = LinearRegression()
model.fit(X, y_observed)

# Create a meshgrid for the plane
x1_range = np.linspace(min(X['BIL']), max(X['BIL']), 100)
x2_range = np.linspace(min(X['ALT']), max(X['ALT']), 100)
x1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)
plane = model.intercept_ + model.coef_[0] * x1_mesh + model.coef_[1] * x2_mesh

# Visualize the 3D scatter plot
fig = plt.figure(figsize=(12, 15))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of observed values
ax.scatter(X['BIL'], X['ALT'], y_observed, color='blue', label='Observed')

# Scatter plot of predicted values
ax.scatter(X['BIL'], X['ALT'], y_predicted, color='red', label='Predicted')

# Plot the best fit plane
ax.plot_surface(x1_mesh, x2_mesh, plane, alpha=0.5, color='green')

ax.set_xlabel('BIL')
ax.set_ylabel('ALT')
ax.set_zlabel('PROT')
ax.set_title('Multiple Linear Regression Visualization for Testing Data')

# Add legend with explicit labels
ax.legend(['Observed', 'Predicted'])
plt.show()
DF2 = pd.DataFrame({'BIL':data['BIL'],'ALT':data['ALT'],'PROT':data['PROT']})
def multiple_linear_regression_manually(df, target):
    x = df.drop(columns=[target]).values
    y = df[target].values
    x = np.c_[np.ones(x.shape[0]), x] #column of 1s inputted for intercept term in X (design matrix)
    coefficients = np.linalg.inv(x.T @ x) @ x.T @ y
    return coefficients
target = 'PROT'
coefficients = multiple_linear_regression_manually(DF2, target)
print("Coefficients:", coefficients)
-------------------------------------------------------------------------------
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=33)
from sklearn.preprocessing import PolynomialFeatures
p=PolynomialFeatures(degree=2,include_bias=True)
x_train_p=p.fit_transform(x_train)
x_test_p=p.transform(x_test)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train_p,y_train)

print("Intercept : ",model.intercept_)
print("Coefficients : ",model.coef_)
y_pred = model.predict(x_test)
r2=r2_score(y_test,y_pred)
r2
plt.scatter(x_train,model.predict(x_train_p))
plt.scatter(x_train,y_train)
plt.show() #bad data no plot
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
model = Ridge()
parameters = {'alpha':[0.001,0.01,1,5,10,20,30,35,40,45,50,55,100,150]}
model_reg = GridSearchCV(model,parameters,scoring='neg_mean_squared_error',cv=5)
model_reg.fit(x_train,y_train)

print(model_reg.best_params_)
print(model_reg.best_score_)
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
lasso=Lasso()
parameters={'alpha':[0.001,0.01,1,5,10,20,30,35,40,45,50,55,100,150]}
lasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)
lasso_regressor.fit(x_train,y_train)

print(lasso_regressor.best_params_)
print(lasso_regressor.best_score_)
regression_types = ["Simple Linear", "Multiple Linear", "Polynomial", "Ridge", "Lasso"]
mse_values = [24.98, 25.15, 20.08, 20.01, 19.80]
plt.figure(figsize=(10, 6))
plt.bar(regression_types, mse_values, color='skyblue')
plt.title('Mean Squared Error for Different Regression Types')
plt.xlabel('Regression Types')
plt.ylabel('Mean Squared Error')
plt.ylim(0, max(mse_values) + 2)
for i, value in enumerate(mse_values):
    plt.text(i, value + 0.5, f'{value:.2f}', ha='center', va='bottom')
plt.show()
---------------------------------------------------------------------------
