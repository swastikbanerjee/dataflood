import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
data = pd.read_csv("/content/program_5_dataset.csv")
data.shape

data.head()
data.isnull().sum()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Sex']=le.fit_transform(data['Sex'])
# data.replace({'Sex':{'m':0,'f':1}},inplace=True)
data.head()
data['ALB'].fillna(data['ALB'].mean(),inplace=True)
data['ALP'].fillna(data['ALP'].mode()[0],inplace=True)
data['ALP'].mode()[0]
data.head()
data = data.dropna()
data.isnull().sum()
data.head()
data.shape
df1 = data.drop(columns=['ALT','CHE'],axis=1)
df1.head()
df1['Sex'].value_counts()
x=df1['ALP']
y=df1['PROT']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=33)
mean_value = x_train.mean()
mean_value=np.array(mean_value)
plt.scatter(x_train,y_train,label='Actual DataPoints',color='purple',marker="^")
plt.axhline(y=mean_value,color='red',label='MeanValue of Target')
plt.xlabel('ALP')
plt.ylabel('PROT')
plt.annotate('Mean Line',xy=(150,mean_value+1))
plt.title('TrainData wrt MeanLine')
plt.legend()
plt.show()
# n=len(x)
# split = int(0.7*n)
# x_train, x_test = x[:split],x[split:]
# y_train, y_test = y[:split],y[split:]
xy = x_train*y_train
n=len(x_train)

x_mean=x_train.mean()
y_mean=y_train.mean()

numerator = xy.sum()-(n*x_mean*y_mean)
denominator = (x_train**2).sum() - (n*(x_mean**2))

m = numerator/denominator
c = y_mean - (m*x_mean)

print("Intercept : ",c)
print("Coefficient : ",m)
x_train_a=np.array(x_train).reshape(-1,1)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train_a,y_train)

print("Intercept : ",model.intercept_)
print("Coefficient : ",model.coef_)
n=len(x_train)
df2 = df1.iloc[:n,:]
y_pred = c+m*df1.iloc[:n,5]
df2['PredPROT']=y_pred
df2[['ALP','PROT','PredPROT']]
fig,ax=plt.subplots()
ax.scatter(x='ALP',y='PROT',data=df2,label='Actual Values')
ax.plot(df2['ALP'],df2['PredPROT'],color='red',label='BestFitTrainingLine')
ax.set_xlabel('ALP')
ax.set_ylabel('PROT')
ax.set_title("BestFitModelLine on Training Data")
ax.legend()
plt.show()
df3 = pd.DataFrame({'ALP':df2['ALP'],'PROT':df2['PROT'],'PredPROT':df2['PredPROT'],'Error':(df2.PredPROT-df2.PROT)})
df3.head()
MSE = (df3['Error']**2).sum()/n
MSE
SSE = (df3['Error']**2).sum()
SST = ((df3['PROT']-df3['PROT'].mean())**2).sum()
SSR = SST - SSE
R2 = SSR/SST
R2
from sklearn.metrics import r2_score
adjusted_r2 = 1 - (1 - r2_score(df3['PROT'], df3['PredPROT'])) * (len(df3['PROT']) - 1) / (len(df3['PROT']) - df3.shape[1] - 1)
adjusted_r2
n=len(x)-len(x_train)
df4 = df1.iloc[n:,:]
y_pred = c+m*df1.iloc[n:,5]
df4['PredPROT']=y_pred
df4[['ALP','PROT','PredPROT']]
fig,ax=plt.subplots()
ax.scatter(x='ALP',y='PROT',data=df4,label='Actual Values')
ax.plot(df2['ALP'],df2['PredPROT'],color='red',label='BestFitTestingLine')
ax.set_xlabel('ALP')
ax.set_ylabel('PROT')
ax.set_title("BestFitModelLine on Testing Data")
ax.legend()
plt.show()
df5 = pd.DataFrame({'ALP':df4['ALP'],'PROT':df4['PROT'],'PredPROT':df4['PredPROT'],'Error':(df4.PredPROT-df4.PROT)})
df5.head()
y_pred = model.predict(np.array(x_test).reshape(-1,1))
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
adjusted_r2 = 1 - (1 - r2_score(df3['PROT'], df3['PredPROT'])) * (len(df5['PROT']) - 1) / (len(df5['PROT']) - df5.shape[1] - 1)
print(f'Mean Squared Error (MSE): {mse}\nR-squared (R2): {r2}\nAdjusted R-squared: {adjusted_r2}')
--------------------------------------------------------------------------------
from statsmodels.stats.outliers_influence import variance_inflation_factor
x = data[['Age','Sex','ALT','BIL']]
y = data['PROT']
vif = pd.Series([variance_inflation_factor(x.values,idx)
                 for idx in range(x.shape[1])],
                index=x.columns)
print(vif)
from statsmodels.stats.outliers_influence import variance_inflation_factor
x = data[['BIL','ALT']]
y = data['PROT']
vif = pd.Series([variance_inflation_factor(x.values,idx)
                 for idx in range(x.shape[1])],
                index=x.columns)
print(vif)
np.corrcoef(data['BIL'],data['ALT'])
plt.scatter(data['BIL'],data['PROT'],color='green')
plt.xlabel('BIL')
plt.ylabel('PROT')
plt.title("Association btw BIL and PROT")
plt.show()
plt.scatter(data['ALT'],data['PROT'],color='purple')
plt.xlabel('ALT')
plt.ylabel('PROT')
plt.title("Association btw ALT and PROT")
plt.show()
x=data[['BIL','ALT']]
y=data['PROT']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)

print("Intercept : ", model.intercept_)
print("Coefficients : ", model.coef_)
y_pred = model.predict(x_test)
n=len(x_train)
DF = data.iloc[n:,:]
DF['PredPROT'] = y_pred
DF.head()
DF
DF1 = pd.DataFrame({'BIL':DF['BIL'],'ALT':DF['ALT'],'PROT':DF['ALT'],'PredPROT':DF['PredPROT'],'Error':(DF.PredPROT-DF.PROT)})
DF1.head()
from sklearn.metrics import r2_score, mean_squared_error
r2 = r2_score(y_test,y_pred)
mse = mean_squared_error(y_test,y_pred)
adj_r2 = 1 - (1 - r2_score(DF1['PROT'],DF1['PredPROT']))*(len(DF1['PROT'])-1)/len(DF1['PROT']-DF1.shape[1]-1)
print(f"R2 : {r2}, MSE : {mse}, ADJ.R2 : {adj_r2}")
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Assuming DF1 contains the data and 'BIL' and 'ALT' are the predictor variables
X = DF1[['BIL', 'ALT']]
y_observed = DF1['PROT']
y_predicted = DF1['PredPROT']

# Fit a linear regression model
model = LinearRegression()
model.fit(X, y_observed)

# Create a meshgrid for the plane
x1_range = np.linspace(min(X['BIL']), max(X['BIL']), 100)
x2_range = np.linspace(min(X['ALT']), max(X['ALT']), 100)
x1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)
plane = model.intercept_ + model.coef_[0] * x1_mesh + model.coef_[1] * x2_mesh

# Visualize the 3D scatter plot
fig = plt.figure(figsize=(12, 15))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of observed values
ax.scatter(X['BIL'], X['ALT'], y_observed, color='blue', label='Observed')

# Scatter plot of predicted values
ax.scatter(X['BIL'], X['ALT'], y_predicted, color='red', label='Predicted')

# Plot the best fit plane
ax.plot_surface(x1_mesh, x2_mesh, plane, alpha=0.5, color='green')

ax.set_xlabel('BIL')
ax.set_ylabel('ALT')
ax.set_zlabel('PROT')
ax.set_title('Multiple Linear Regression Visualization for Testing Data')

# Add legend with explicit labels
ax.legend(['Observed', 'Predicted'])
plt.show()
DF2 = pd.DataFrame({'BIL':data['BIL'],'ALT':data['ALT'],'PROT':data['PROT']})
def multiple_linear_regression_manually(df, target):
    x = df.drop(columns=[target]).values
    y = df[target].values
    x = np.c_[np.ones(x.shape[0]), x] #column of 1s inputted for intercept term in X (design matrix)
    coefficients = np.linalg.inv(x.T @ x) @ x.T @ y
    return coefficients
target = 'PROT'
coefficients = multiple_linear_regression_manually(DF2, target)
print("Coefficients:", coefficients)
-------------------------------------------------------------------------------
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=33)
from sklearn.preprocessing import PolynomialFeatures
p=PolynomialFeatures(degree=2,include_bias=True)
x_train_p=p.fit_transform(x_train)
x_test_p=p.transform(x_test)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train_p,y_train)

print("Intercept : ",model.intercept_)
print("Coefficients : ",model.coef_)
y_pred = model.predict(x_test)
r2=r2_score(y_test,y_pred)
r2
plt.scatter(x_train,model.predict(x_train_p))
plt.scatter(x_train,y_train)
plt.show() #bad data no plot
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
model = Ridge()
parameters = {'alpha':[0.001,0.01,1,5,10,20,30,35,40,45,50,55,100,150]}
model_reg = GridSearchCV(model,parameters,scoring='neg_mean_squared_error',cv=5)
model_reg.fit(x_train,y_train)

print(model_reg.best_params_)
print(model_reg.best_score_)
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
lasso=Lasso()
parameters={'alpha':[0.001,0.01,1,5,10,20,30,35,40,45,50,55,100,150]}
lasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)
lasso_regressor.fit(x_train,y_train)

print(lasso_regressor.best_params_)
print(lasso_regressor.best_score_)
regression_types = ["Simple Linear", "Multiple Linear", "Polynomial", "Ridge", "Lasso"]
mse_values = [24.98, 25.15, 20.08, 20.01, 19.80]
plt.figure(figsize=(10, 6))
plt.bar(regression_types, mse_values, color='skyblue')
plt.title('Mean Squared Error for Different Regression Types')
plt.xlabel('Regression Types')
plt.ylabel('Mean Squared Error')
plt.ylim(0, max(mse_values) + 2)
for i, value in enumerate(mse_values):
    plt.text(i, value + 0.5, f'{value:.2f}', ha='center', va='bottom')
plt.show()
---------------------------------------------------------------------------

Program 3a
Importing Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
Importing datasets
dfa = pd.read_csv("/content/Program_3a_Dataset.csv")
dfb = pd.read_csv("/content/Program_3b_dataset.csv")
dfa.head()
dfa.describe()
dfa.isnull().sum()
No missing values. Hence our dataframe is clean.
Train-Test Split
from sklearn.model_selection import train_test_split
x = dfa[['duration','age','campaign']]
y = dfa['y']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
Fitting Logistic Regression model to our training dataframe
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(x_train, y_train)
Accuracy Score on training data
from sklearn.metrics import accuracy_score
x_train_prediction = log_reg.predict(x_train)
training_data_accuracy = accuracy_score(x_train_prediction, y_train)
training_data_accuracy*100
Accuracy Score on test data
x_test_prediction = log_reg.predict(x_test)
test_data_accuracy = accuracy_score(x_test_prediction, y_test)
test_data_accuracy*100
y_pred = log_reg.predict(x_test)
Classification Report (Precision, Recall, F1-Score, Accuracy) of the model
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))
Confusion Matrix of the model
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,linewidths=2,cmap='inferno',fmt='4')
plt.show()
Inference : For the combination of predictors we have used - Duration, Age and Campaign, we got a 89% accuracy score to predict whether the client will subscribe to a term deposit or not (both for training and test dataset). The evaluation metric scores for positive case came really poor. We need more data or upsampling for positive cases is required, for improving classification scores for this model.
Program 3b
dfb.head()
dfb.describe()
dfb.isnull().sum()
No missing values. Hence our dataframe is clean.
Train-Test Split
from sklearn.model_selection import train_test_split
x = dfb[['variance','skewness','curtosis','entropy']]
y = dfb['class']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
Fitting Logistic Regression model to our training dataframe
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(random_state=44)
log_reg.fit(x_train, y_train)
Accuracy Score on training data
from sklearn.metrics import accuracy_score
x_train_prediction = log_reg.predict(x_train)
training_data_accuracy = accuracy_score(x_train_prediction, y_train)
training_data_accuracy*100
Accuracy Score on test data
x_test_prediction = log_reg.predict(x_test)
test_data_accuracy = accuracy_score(x_test_prediction, y_test)
test_data_accuracy*100
y_pred = log_reg.predict(x_test)
Classification Report (Precision, Recall, F1-Score, Accuracy) of the model
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))
Confusion Matrix of the model
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,linewidths=2,cmap='inferno',fmt='4')
plt.show()
Inference : For the combination of predictors we have used - variance, skewness, curtosis, entropy we got a 99% accuracy score to predict whether the client will subscribe to a term deposit or not (both for training and test dataset). Evaluation metric scores for both positive and negative scores case came really good. The classes were properly balanced, model performed well on test data, very less False Positive and False Positive Values were seen in the Confusion Matrix.